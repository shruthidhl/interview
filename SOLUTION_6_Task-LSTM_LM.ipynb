{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1LpjNSceR85m"
   },
   "source": [
    "# LSTM in practice -- NLP\n",
    "\n",
    "## Language modeling\n",
    "\n",
    "A language model is a probability distribution over the sequence of words, modeling language (production), thus if the set of words is $w$, then for arbitrary $\\mathbf w = \\langle w_1,\\dots, w_n\\rangle$ ($w_i\\in W$) sequence it defines a $P(\\mathbf w)$ probability. \n",
    "\n",
    "Probability with chain rule:\n",
    "\n",
    "$$P(\\mathbf w)= P(w_1)\\cdot P(w_2 \\vert w_1 )\\cdot P(w_3\\vert w_1, w_2)\\cdot\\dots\\cdot P(w_n\\vert w_1,\\dots, w_{n-1})$$\n",
    "\n",
    "$$P(This\\;is\\;a\\;nice\\;house)= P(This)\\cdot P(is \\vert This )\\cdot P(a\\vert This, is) \\cdot P(nice\\vert This, is, a)\\cdot P(house\\vert This, is, a, nice)$$\n",
    "\n",
    "so this means, that for the modeling we need only to give the conditional probability of the \"continuation\", the next word, thus for $w$ word and $\\langle w_1,\\dots,w_n\\rangle$ sequence the probability that the next word will be $w$\n",
    "\n",
    "$$P(w ~\\vert ~ w_1,\\dots,w_n)$$\n",
    "\n",
    "There are character based models also, which take the individual characters as units, not the words, and model language as a distribution over sequences of characters (think T9...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "moYnnb1-jKT4"
   },
   "source": [
    "### Measurement of performance: Perplexity\n",
    "\n",
    "First, we should split our original corpus $w=\\langle w_1,\\dots, w_n\\rangle$ to train $w_{train}=\\langle w_1,\\dots, w_k\\rangle$ and test data $w_{test}=\\langle w_{k+1},\\dots, w_n\\rangle$\n",
    "\n",
    "A language model $\\mathcal M$'s perplexity over the word series $\\mathbf w_{test} = \\langle w_{k+1},\\dots, w_n\\rangle$ is:\n",
    "\n",
    "$$\\mathbf{PP}_{\\mathcal M}(\\mathbf w_{test}) = \\sqrt[n-k]{\\frac{1}{P_{\\mathcal M}(\\mathbf w_{test})}}$$\n",
    "\n",
    "With the chain rule can be rewritten as:\n",
    "\n",
    "$$\\mathbf{PP}_{\\mathcal M}(\\mathbf w_{test}) = {\\sqrt[n-k]{\\frac{1}{P_{\\mathcal M}(w_{k+1})}\\cdot \\frac{1}{P_{\\mathcal M}(w_{k+2} \\vert w_{k+1} )}\\cdot \\frac{1}{P_{\\mathcal M}(w_{k+3}\\vert w_{k+1}, w_{k+2})}\\cdot\\dots\\cdot \\frac{1}{P_{\\mathcal M}(w_n\\vert w_{k+1},\\dots, w_{n-1})}}}$$\n",
    "\n",
    "which is exactly the geometric mean of the reciprocals of the conditional probabilities of all words in the corpus.\n",
    "\n",
    "In case of a bigram model this is further simplified to:\n",
    "$$\\mathbf{PP}_{\\mathcal M}(\\mathbf w_{test}) = \\sqrt[n-k]{\\frac{1}{P_{\\mathcal M}(w_{k+1})}\\cdot \\frac{1}{P_{\\mathcal M}(w_{k+2} \\vert w_{k+1} )}\\cdot \\frac{1}{P_{\\mathcal M}(w_{k+3}\\vert w_{k+2})}\\cdot\\dots\\cdot \\frac{1}{P_{\\mathcal M}(w_n\\vert w_{n-1})}}$$\n",
    "\n",
    "Example for a bigram model perplexity:\n",
    "- **Train corpus**: \"This is the house that Jack built. What a house. Jack is a genius.\"\n",
    "- **Test corpus**: \"This is a house.\"\n",
    "\n",
    "$$\\mathbf{PP}_{\\mathcal M}(This\\;is\\;a\\;house) = \\sqrt[4]{\\frac{1}{P_{\\mathcal M}(This)}\\cdot \\frac{1}{P_{\\mathcal M}(is \\vert This)}\\cdot \\frac{1}{P_{\\mathcal M}(a \\vert is)}\\cdot \\frac{1}{P_{\\mathcal M}(house\\vert a)}}=$$\n",
    "\n",
    "$$ = \\sqrt[4]{\\frac{1}{0.33}\\cdot \\frac{1}{1}\\cdot \\frac{1}{0.5}\\cdot \\frac{1}{0.5}} \\approx 1.86$$\n",
    "\n",
    "The **lower** perplexity the better.\n",
    "\n",
    "[source](https://www.youtube.com/watch?v=oaYsCVtHveQ)\n",
    "\n",
    "### But what is it good for?\n",
    "For example:\n",
    "- Predictive text input (\"autocomplete\")\n",
    "- Generating text\n",
    "- Spell checking\n",
    "- Language understanding\n",
    "- And most importantly representation learning - this we will be studiying in detail in a next lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fn4iPehcyda2"
   },
   "source": [
    "### Generating text with a language model\n",
    "\n",
    "The language model produces a tree with probable continuations of the text:\n",
    "\n",
    "<img src=\"https://4.bp.blogspot.com/-Jjpb7iyB37A/WBZI4ImGQII/AAAAAAAAA9s/ululnUWt2vw9NMKuEr-F9H8tR2LEv36lACLcB/s1600/prefix_probability_tree.png\" width=400 heigth=400>\n",
    "\n",
    "Using this tree we can try different algorithms to search for the best \"continuations\". A full breadth-first search oi usually impossible, due to the high branching factor of the tree.\n",
    "\n",
    "Alternatives:\n",
    "- \"Greedy\": we choose the continuation which has the highest direct probability, This will most probably be suboptimal, since the probability of the full sequence is tha product of the continuations, and if we would have chosen a different path, we might ahve been able to choose later words with hihg probabilities.\n",
    "- Beam-search: we always store a fixed $k$ number of partial sequences, and we always try to expand these, always keeping the most probable $k$ from the possible continuations. \n",
    "\n",
    "Example ($k$=5):\n",
    "\n",
    "<img src=\"http://opennmt.net/OpenNMT/img/beam_search.png\" width=600 heigth=600>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pO8BuLsDWWAE"
   },
   "source": [
    "### The \"old way\": N-gram based solutions\n",
    "\n",
    "With _gross_ simplification we assume, that the distribution is only dependent on the prior $n-1$ words (where $n$ is typically $<=4$), thus we assume a Markov chain of the order $n$:\n",
    "\n",
    " $$P(w ~\\vert ~ w_1,\\dots,w_k) = P(w ~\\vert ~ w_{k- n + 2},\\dots,w_k)$$\n",
    " \n",
    "Example for 2-gram or Bigram language model:\n",
    "\n",
    "$$Toy\\;corpus:This\\;is\\;the\\;malt.\\;That\\;lay\\;in\\;the\\;house\\;that\\;Jack\\;built.$$\n",
    "\n",
    "$$P(This\\;is\\;the\\;house) = P(This)\\cdot P(is\\vert This)\\cdot P(the\\vert is)\\cdot P(house\\vert the)$$\n",
    "\n",
    "[Source](https://www.youtube.com/watch?v=GiyMGBuu45w)\n",
    "\n",
    "We simply compute these probabilities in a frequentist style by calculating the $n$-gram statistics of the corpus at hand:\n",
    "\n",
    "$$P(w_2 ~\\vert ~w_1) = \\frac{c(\\langle w_1, w_2 \\rangle)}{c(w_1)}$$\n",
    "\n",
    "$$P(Jack \\vert that)=\\frac{c(that, Jack)}{c(that)}=\\frac{1}{2}=50\\%$$\n",
    "\n",
    "$$P(w_{k+1} \\vert~ w_1,\\dots,w_k)_\\mathrm = \\frac{c(\\langle w_1,...,w_k, w_{k+1} \\rangle)}{c(\\langle w_1, \\dots w_k\\rangle)}$$\n",
    "\n",
    "Please note, that in this case we are using \"memorization\", a form of database learning, with minimal compression - \"counting\".\n",
    "\n",
    "But what do we do the given $n$-grams rarely or never occur? We have to employ some __smoothing__ solutions, like: \n",
    "\n",
    "##### Additive smoothing\n",
    "We pretend that we have seen the $n$-grams more times than we have actually did with a fixed $\\delta$ number, in the simplest case with $n=2$:\n",
    "\n",
    "$$P(w_2 ~\\vert ~w_1) = \\frac{c(\\langle w_1, w_2 \\rangle) + \\delta}{\\sum_{w\\in V} [c(\\langle w_1, w\\rangle) + \\delta]}$$\n",
    "\n",
    "Widespread solution for $\\delta$ is $1$.\n",
    "\n",
    "The main problem with this kind of smoothing is that it does not take into account by \"supplementing\" the data the frequency of components of shorter $n$-grams, eg. if neither $\\langle w_1, w_2 \\rangle$  nor $\\langle w_1, w_3 \\rangle$ occurs in the corpus, it assumes the frequency of both bigrams to be $\\delta$, irrespective of the ratio of frequencies of $w_2$ and $w_3$.\n",
    "\n",
    "Most smoothing techniques are trying to accomodate this, eg: simple interpolation:\n",
    "\n",
    "##### Interpolatcion\n",
    "\n",
    "In case of bigrams, we add - with a certain weight - the probabilities coming from the individual frequencies:\n",
    "\n",
    "$$P(w_2 ~\\vert ~w_1)_{\\mathrm{interp}} = \\lambda_1\\frac{c(\\langle w_1, w_2 \\rangle)}{c(w_1)} + (1 - \\lambda_1)\\frac{c(w_1)}{\\sum_{w\\in V}c(w)}$$\n",
    "\n",
    "Recursive solution for arbitrary $k$:\n",
    "\n",
    "$$P(w_{k+1} \\vert~ w_1,\\dots,w_k)_\\mathrm{interp} = \\lambda_k\\frac{c(\\langle w_1,...,w_k, w_{k+1} \\rangle)}{c(\\langle w_1, \\dots w_k\\rangle)} + (1-\\lambda_k)P_\\mathrm{interp}(\\langle w_2,\\dots,w_{k+1}\\rangle)$$\n",
    "\n",
    "$\\lambda_k$ is empirically set by examining the corpus, typically by [Expectation Maximization algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm), which - as we have mentioned - iteratively tunes the parameters to maximize the likelihood.\n",
    "\n",
    "\n",
    "Good overview about the smoothing methods: [MacCartney, NLP Lunch Tutorial: Smoothing](https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf)\n",
    "\n",
    " \n",
    "#### General problems\n",
    "\n",
    "- Even the core assumption is not too realistic, since the probabilities are for sure influenced in a way by words further than $n$, but for practical reasons, it has to be limited (sparsity, computation capacity).\n",
    "- On a large enough corpus, the memory footprint of the $n$-gram models is _huge_, eg. for the 1T n-gram corpus of Google ([see here](https://catalog.ldc.upenn.edu/LDC2006T13)) containing 1,024,908,267,229 tokens the $n$-gram counts are as follows:\n",
    "    - unigram: 13,588,391, \n",
    "    - bigram: 314,843,401, \n",
    "    - trigram: 977,069,902, \n",
    "    - fourgrams: 1,313,818,354 \n",
    "    - fivegram: 1,176,470,663."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DgLdc8JQrH-4"
   },
   "source": [
    "## Language modeling with LSTMs\n",
    "\n",
    "One way to circumvent the Markov assumption is to use RNN-s, which are capable of modeling the long-ter dependencies inside the sequence of words. The text is thus considered to be a time-series, and thus an appropriate architecture can be used (as we have already seen):\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1y8QYr9ftTvXAxgzS-ldnGlijVpmK2l21\" width=600 heigth=600>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YHVR_jd62LHI"
   },
   "source": [
    "Notable features:\n",
    "\n",
    "- Input is a \"one-hot\" encoded vector, wchic we on the spot transform into an \"embedding vector\"\n",
    "- For each output step, we get a probability distribution over the whole vocabulary with softmax\n",
    "- This above is a simple RNN, but LSTMs can be used without any problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wqQY-P9XP6Ro"
   },
   "source": [
    "### Teaching\n",
    "\n",
    "_In theory_ an RNN could be trained with full GD on the corpus in one go:\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1XsBoRp7cNay3svFLRDv2JEDyC7m7CUdC\" width=600 heigth=600>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lXIBEAhM8dzU"
   },
   "source": [
    "- The loss is generally the well-kown crossentropy, which is in this case (since the input is a one-hot vector):\n",
    "  $$J^{(i)}(\\Theta) = -\\log (\\hat y[x^{(i+1)}])$$\n",
    "  the negative logarithm of the probability assigned by the network to the right word / next word.\n",
    "\n",
    "- For the sake of more frequent updates, and since BPTT for long sequences is very expensive, teaching is done in smaller units with not necessarily the same length.\n",
    "- The unit is typically one or more sentece, or if the length allows, and we have enough material, a paragraph can be a good candidate.\n",
    "- Initial state in case of the time-series units: if the boundaries are inside a unit of text, it is important to _transfer the hidden state_ from the previous unit, in other cases initialization can be done by some fixed value.\n",
    "- (Somewhat misleading) terminology: the length of the \"time\" unit is _time step_, but sometimes certain implementations call it _minibatch_, though that would generally mean the number of units processed in one go for the sake of computaitonal efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R8IYutdnW_SG"
   },
   "source": [
    "### LSTM as layers\n",
    "\n",
    "+ An LSTM - how ever strange that may sound - can be considered to be a complete layer. The most important parameter of it is the \"number of (memory) units\", which is the length of the hidden state vector, thus, the memory capacity. **Warning: this does not have any relationship to input size, thus can be considered a freely chosen parameter.**\n",
    "+ It is quite widespread to use multiple LSTM layers (\"stacked LSTMs\") -- as in the case of ConvNets the hope is, that the layers learn a hierarchy of abstract representations:\n",
    "\n",
    "<img src=\"http://wenchenli.github.io/assets/img/GNMT_residual.png\" width=60%>\n",
    "\n",
    "(on the right side a network is shown with skip/residual connections!)\n",
    "\n",
    "In this case it makes sense, that we do not only get on top of the LSTM a final prediction $h$ (or even prediction + inner state vector $c$) for a sequence, but **we ask it to output the whole sequence of predictions**, so that the next layer can also operate on full sequences. Please bear this in mind during implementation, since this can be a common source of failure.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3R04Si0M8KzK"
   },
   "source": [
    "## An LSTM language model in TF\n",
    "\n",
    "For this task the inspiration comes from the famous [reference work of Andrej Karpathy](https://karpathy.github.io/2015/05/21/rnn-effectiveness/). \n",
    "\n",
    "Note, that in this case we will not use regularization, since we are willing to overfit - for the sake of play with the text. This is now an \"overfitting competition\", so _not_ a generally good practice!\n",
    "\n",
    "## Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-06T16:48:43.620252Z",
     "start_time": "2018-10-06T16:48:43.607851Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "egfH6gkf8WQg",
    "outputId": "34be7900-04c0-4415-9559-c7728cf3bd96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/simka/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1212)\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "nltk.download(\"brown\")\n",
    "\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# This can be an important parameter, so be aware of it...\n",
    "max_seq_length = 15\n",
    "\n",
    "# max_num_of_sents -- how many sentences should we read from the corpus (max=57200)\n",
    "max_num_of_sents = 50 # 57200\n",
    "\n",
    "def generate_brown_word_to_id_map():\n",
    "    \"\"\"Return a dictionary mapping downcased Brown-words to their ids.\n",
    "    Numbering starts from 1 since we use 0 for masking (!!!).\n",
    "    \"\"\"\n",
    "    words = set()\n",
    "    for word in brown.words():\n",
    "        words.add(word.lower())\n",
    "    return {word: idx + 1 for idx, word in enumerate(sorted(words))}\n",
    "\n",
    "\n",
    "class BrownReader:\n",
    "    \"\"\"A reader class for the Brown corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.word_to_id_map = generate_brown_word_to_id_map()\n",
    "        self.id_to_word_map = {idx: word for word, idx in self.word_to_id_map.items()}\n",
    "\n",
    "    def n_words(self):\n",
    "        return len(self.word_to_id_map)\n",
    "\n",
    "    def sentence_to_ids(self, sentence):\n",
    "        \"\"\"Return the word ids of a sentence.\n",
    "        \"\"\"\n",
    "        return [self.word_to_id_map[word.lower()] for word in sentence]\n",
    "        \n",
    "    def sentences(self):\n",
    "        \"\"\"Generator yielding features from the Brown corpus.\n",
    "        \"\"\"\n",
    "        return (self.sentence_to_ids(sentence) for sentence in brown.sents())\n",
    "\n",
    "    def sentence_matrixes(self):\n",
    "        x = np.zeros((max_num_of_sents, max_seq_length-1))\n",
    "        y = np.zeros((max_num_of_sents, max_seq_length-1))\n",
    "        sents = self.sentences()\n",
    "        for idx, sent in enumerate(sents):\n",
    "            if idx == max_num_of_sents:\n",
    "                break\n",
    "            np_array = np.asarray(sent)\n",
    "            length  = min(max_seq_length, len(np_array))\n",
    "            x[idx, :length - 1] = np_array[:length - 1]\n",
    "            y[idx, :length - 1] = np_array[1:length]\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pvjdF2_p9_YV"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SavNwc5m9_Ya"
   },
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "CXHNzCXj9_Ye"
   },
   "outputs": [],
   "source": [
    "br = BrownReader()\n",
    "n_words = br.n_words()\n",
    "\n",
    "# network parameters\n",
    "\n",
    "lstm_size = 512\n",
    "embedding_size = 150\n",
    "max_input_length = max_seq_length - 1 # since our x/y input does not contain the last/first element of the sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GWPOCc6q9_Yn"
   },
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "id": "FXth0CDm9_Yq",
    "outputId": "0c8edcf1-b6a8-49bb-8726-23fb7e14b17f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x7fa1e36bae80>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/simka/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1455, in __del__\n",
      "    self._session._session, self._handle, status)\n",
      "  File \"/home/simka/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 528, in __exit__\n",
      "    c_api.TF_GetCode(self.status.status))\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: No such callable handle: 140332687733120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 14, 150)           7472400   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 14, 512)           1357824   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                [(None, 14, 512), (None,  2099200   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 14, 49816)         25555608  \n",
      "=================================================================\n",
      "Total params: 36,485,032\n",
      "Trainable params: 36,485,032\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adadelta, Adam, SGD\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "tf.compat.v1.reset_default_graph() # It's good practice to clean and reset everything\n",
    "K.clear_session            # even using Keras\n",
    "\n",
    "\n",
    "# Model\n",
    "########\n",
    "\n",
    "x = Input(shape=(max_input_length,))\n",
    "\n",
    "embedded_x =  Embedding(n_words + 1, embedding_size, input_length=max_input_length - 1, mask_zero=True)(x)\n",
    "\n",
    "lstm_outputs = LSTM(lstm_size, return_sequences=True)(embedded_x)\n",
    "\n",
    "lstm_outputs, hidden_state, cell_state = LSTM(lstm_size, return_sequences=True, return_state= True)(lstm_outputs)\n",
    "\n",
    "predictions = Dense(n_words + 1, activation=\"softmax\")(lstm_outputs)\n",
    "\n",
    "model = Model(inputs=x, outputs=predictions)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FKXuebno9_Y1"
   },
   "source": [
    "### Error, optimization, compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "4Mob9-6s9_Y3"
   },
   "outputs": [],
   "source": [
    "# Loss \n",
    "\n",
    "loss = sparse_categorical_crossentropy # we use this cross entropy variant as the input is not \n",
    "                                       # one-hot encoded\n",
    "\n",
    "# Optimizer\n",
    "# Choose an optimizer - adaptive ones work well here\n",
    "optimizer = Adam()\n",
    " \n",
    "# Compilation\n",
    "#############\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "JkhDDIwl9_ZC"
   },
   "outputs": [],
   "source": [
    "data_x, data_y = br.sentence_matrixes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WLLqU3c_9_Y8"
   },
   "source": [
    "### Training\n",
    "\n",
    "We generate the trainig data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "rJdzjP6R9_ZK"
   },
   "outputs": [],
   "source": [
    "data_y = np.expand_dims(data_y, -1) # It seems that Keras needs this for the \"one-cold\" and softmax dims to match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NMPFHd-M9_ZX"
   },
   "source": [
    "And train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "colab_type": "code",
    "id": "EJboxaY_9_Zd",
    "outputId": "18ce2f19-4442-4875-9e9d-0383fc12aec5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simka/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45 samples, validate on 5 samples\n",
      "Epoch 1/5\n",
      "45/45 [==============================] - 9s 196ms/step - loss: 9.7203 - val_loss: 9.8397\n",
      "Epoch 2/5\n",
      "45/45 [==============================] - 5s 119ms/step - loss: 9.0516 - val_loss: 9.4030\n",
      "Epoch 3/5\n",
      "45/45 [==============================] - 4s 98ms/step - loss: 8.2130 - val_loss: 9.0863\n",
      "Epoch 4/5\n",
      "45/45 [==============================] - 4s 96ms/step - loss: 7.4374 - val_loss: 8.9445\n",
      "Epoch 5/5\n",
      "45/45 [==============================] - 4s 96ms/step - loss: 6.8902 - val_loss: 8.9446\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=data_x , y=data_y, validation_split=0.1, epochs=5, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eao03t2U9gBy"
   },
   "source": [
    "## Demo 1: Predict next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-06T17:04:58.584741Z",
     "start_time": "2018-10-06T16:48:43.630Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "colab_type": "code",
    "id": "RM1WVrua9qaY",
    "outputId": "b5d6934b-a0ac-4ad8-82a8-44f3e2d703a9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter a few starting words of a sentence or <return> to stop: This\n",
      "Predicted next word: the\n",
      "\n",
      "Enter a few starting words of a sentence or <return> to stop: large\n",
      "Predicted next word: the\n",
      "\n",
      "Enter a few starting words of a sentence or <return> to stop: \n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "############\n",
    "\n",
    "def str_to_input(s):\n",
    "    \"\"\"Convert a string to appropriate model input.\n",
    "    \"\"\"\n",
    "    words = [x.lower() for x in s.split()[:max_input_length]]\n",
    "    ids = [br.word_to_id_map[word] for word in words]\n",
    "    ids_array = np.asarray(ids)\n",
    "    length = min(max_input_length, len(ids_array))\n",
    "    result = np.zeros((1, max_input_length))\n",
    "    result[0, :length] = ids_array[:length]\n",
    "    return result, length\n",
    "    \n",
    "\n",
    "while True:\n",
    "    s = input(\"\\nEnter a few starting words of a sentence or <return> to stop: \")\n",
    "    if s == \"\":\n",
    "        break\n",
    "    else:\n",
    "        try:\n",
    "            x, length = str_to_input(s)\n",
    "            predictions = model.predict(x)\n",
    "            probs = predictions[0][length - 1]\n",
    "            most_probable = np.argmax(probs)\n",
    "            print(\"Predicted next word:\", br.id_to_word_map[most_probable])\n",
    "        except KeyError:\n",
    "            print(\"Unknown words -- please try again!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L4AkEJxBzPfm"
   },
   "source": [
    "## Demo 2: Similarity of sentences\n",
    "\n",
    "First we define a function that generates the hidden state of the LSTM from an input sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = model.get_layer(\"input_1\")\n",
    "lstm_2_layer = model.get_layer(\"lstm_1\")\n",
    "\n",
    "cell_state_fun = K.function([input_layer.input],[lstm_2_layer.output[2]])\n",
    "\n",
    "def get_embedding(x):\n",
    "    \"\"\"Return the final cell state associated with the input.\n",
    "       Returns the last cell state as a vector.\n",
    "    \"\"\"\n",
    "    return cell_state_fun([x])[0].flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the vectors for calculating the cosine distance between sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1317
    },
    "colab_type": "code",
    "id": "LKUgmCrG7Itw",
    "outputId": "211052dd-94d9-415d-975d-63b2d53c8f5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the first sentence or <return> to quit: I love data science\n",
      "\n",
      "Enter the second sentence: I hate data science\n",
      "The cosine similarity between the two sentences is 0.9999893\n"
     ]
    }
   ],
   "source": [
    "def cos_sim(a, b):\n",
    "\treturn np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "while True:\n",
    "    s1 = input(\"\\nEnter the first sentence or <return> to quit: \")\n",
    "    if s1 == \"\": break\n",
    "    s2 = input(\"\\nEnter the second sentence: \")\n",
    "    try:\n",
    "        x1, _ = str_to_input(s1)\n",
    "        x2, _ = str_to_input(s2)\n",
    "        e1 = get_embedding(x1)\n",
    "        e2 = get_embedding(x2)\n",
    "        print(\"The cosine similarity between the two sentences is\", cos_sim(e1, e2))\n",
    "    except KeyError:\n",
    "        print(\"Unknown words -- please try again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1-3W1oxZv4DI"
   },
   "source": [
    "## Demo 3: Mini search engine\n",
    "\n",
    "We use the library [Annoy](https://github.com/spotify/annoy) published by Spotify to create a vector space index of the Brown corpus from the LSTM's cell state. We assign a vector for each sentence, and then store it to be able to run nearest neighbor queries on it. With this we effectively created a **semantic search engine**.\n",
    "\n",
    "(There are multiple solutions for approximate nearest neighbor search a scale which are worth looking into, one of them is [FAISS](https://code.fb.com/data-infrastructure/faiss-a-library-for-efficient-similarity-search/) from Facebook Research.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "MWp-rofueeS0"
   },
   "outputs": [],
   "source": [
    "def brown_sent_to_input(ids):\n",
    "  ids_array = np.asarray(ids)\n",
    "  length = min(max_input_length, len(ids_array))\n",
    "  result = np.zeros((1, max_input_length))\n",
    "  result[0, :length] = ids_array[:length]\n",
    "  return result, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "wda7o8ntfov1"
   },
   "outputs": [],
   "source": [
    "sentlist = list(br.sentences())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "colab_type": "code",
    "id": "eaAJ7Pz5i8Cp",
    "outputId": "f94ff59a-e327-4d02-e725-66ea3e5682fc"
   },
   "outputs": [],
   "source": [
    "!pip install annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "o8SO85y_w8sX"
   },
   "outputs": [],
   "source": [
    "INDEX_COVERAGE_PERCENT = 1.0 #How much of the corpus you want ot index? 1.0 means whole, 0.5 means half.\n",
    "NEAREST_NEIGHBOR_NUM = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "UfwnfqYff6h8",
    "outputId": "930b77b2-3ce1-4b5d-bbd4-03e57b40fdb7"
   },
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "from tqdm import tqdm\n",
    "\n",
    "index = AnnoyIndex(512, metric=\"angular\")\n",
    "\n",
    "for i in tqdm(range(int(len(sentlist)*INDEX_COVERAGE_PERCENT))):\n",
    "  inputs,length = brown_sent_to_input(sentlist[i])\n",
    "  vector = get_embedding(inputs)\n",
    "  index.add_item(i,vector)\n",
    "\n",
    "print(\"Building index...\")\n",
    "index.build(100)\n",
    "print(\"Index done, ready to query!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ex6AGjVrh6Sb"
   },
   "outputs": [],
   "source": [
    "def print_brown_index(sentences, indices):\n",
    "  for i in indices:\n",
    "    word_ids_list = sentences[i]\n",
    "    for j in word_ids_list:\n",
    "      print(br.id_to_word_map[j]+\" \", end='')\n",
    "    print()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 693
    },
    "colab_type": "code",
    "id": "V6y-1aVGi573",
    "outputId": "68d4ef41-2398-4e56-a170-25bae99df5c7"
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "  query = input(\"\\nEnter the query or <return> to quit: \")\n",
    "  if query == \"\": break\n",
    "  try:\n",
    "    in_ids, length = str_to_input(query)\n",
    "    in_vector = get_embedding(in_ids)\n",
    "    nearest_sentence_indices = index.get_nns_by_vector(in_vector, NEAREST_NEIGHBOR_NUM)\n",
    "    #print(\"nearest indices:\", nearest_sentence_indices)\n",
    "    print_brown_index(sentlist, nearest_sentence_indices)\n",
    "\n",
    "  except KeyError:\n",
    "    print(\"Unknown words -- please try again!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM_LM_Task.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
